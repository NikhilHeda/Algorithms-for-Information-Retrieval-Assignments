{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import nltk.data\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "\n",
    "import re\n",
    "\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DESCRIPTION ='''\n",
    "Dataset Description:\n",
    "    .I -> ID\n",
    "    .T -> Title\n",
    "    .A -> Author\n",
    "    .B -> Date\n",
    "    .W -> Actual Text\n",
    "'''\n",
    "\n",
    "FILEPATH = 'Dataset/cran.all.1400'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to parse content of Cranfield Dataset\n",
    "Uses Regex for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    '''\n",
    "    Input: Filename\n",
    "    Output: List of dictionaries\n",
    "        [ {\n",
    "            'ID':\n",
    "            'Title':\n",
    "            'Author':\n",
    "            'Data':\n",
    "            'Content':\n",
    "        }, ...]\n",
    "    '''\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        contents = f.read()\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r\"\"\"\\.I(?P<ID>.*?)\\.T(?P<Title>.*?)\\.A(?P<Author>.*?)\\.B(?P<Date>.*?)\\.W(?P<Content>.*?(?=\\.I))\"\"\",\n",
    "        re.DOTALL\n",
    "    ) \n",
    "    \n",
    "    parsed_content = [\n",
    "        match.groupdict()\n",
    "        for match in re.finditer(pattern, contents + '.I')\n",
    "    ]\n",
    "    \n",
    "    return parsed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse_file(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    \n",
    "    def __init__(self, language):\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.punctuations = set(string.punctuation)\n",
    "\n",
    "    def get_tokens(self, data):\n",
    "        return word_tokenize(data)\n",
    "    \n",
    "    def stopword_removal(self, data):\n",
    "        return list(filter(lambda word: word not in self.stop_words, data))\n",
    "\n",
    "    def stemming(self, word):\n",
    "        porter_stemmer = PorterStemmer()\n",
    "        return porter_stemmer.stem(word)\n",
    "\n",
    "    def normalize(self, word):\n",
    "        word = str.lower(word)\n",
    "        \n",
    "        word = ''.join([letter for letter in word if letter not in self.punctuations ])\n",
    "        \n",
    "        word = stemming(word)\n",
    "        return word\n",
    "    \n",
    "    def validate(self, data):\n",
    "        return list(filter(lambda word: word != '', data))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocess = PreProcessor('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class myThread (threading.Thread):\n",
    "\n",
    "    def __init__(self, threadID, name, begin, end):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.threadID = threadID\n",
    "        self.name = name\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "\n",
    "    def run(self):\n",
    "        preprocess_documents(self.begin, self.end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_documents(begin, end):\n",
    "    while begin != end + 1:\n",
    "        for key in parsed[begin]:\n",
    "            data = parsed[begin][key]\n",
    "            data = preprocess.get_tokens(data)\n",
    "            if key == 'Content':\n",
    "                data = preprocess.stopword_removal(data)\n",
    "                data = list(map(lambda word: preprocess.normalize(word), data))\n",
    "                data = preprocess.validate(data)\n",
    "            parsed[begin][key] = data\n",
    "        begin += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = [myThread(i, \"Thread -\" + str(i), 14*i, 14*i + 13) for i in range(100)]\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID': ['1400'], 'Title': ['the', 'buckling', 'shear', 'stress', 'of', 'simply-supported', 'infinitely', 'long', 'plates', 'with', 'transverse', 'stiffeners', '.'], 'Author': ['kleeman', ',', 'p.w', '.'], 'Date': ['arc', 'r', '+', 'm.2971', ',', '1953', '.'], 'Content': ['buckl', 'shear', 'stress', 'simplysupport', 'infinit', 'long', 'plate', 'transvers', 'stiffen', 'report', 'extens', 'previou', 'theoret', 'investig', 'elast', 'buckl', 'shear', 'flat', 'plate', 'reinforc', 'transvers', 'stiffen', 'plate', 'treat', 'infinit', 'long', 'simplysupport', 'along', 'long', 'side', 'stiffen', 'space', 'regular', 'interv', 'divid', 'plate', 'number', 'panel', 'uniform', 'size', 'effect', 'ob', 'bend', 'torsion', 'stiff', 'stiffen', 'upon', 'buckl', 'shear', 'stress', 'calcul', 'complet', 'rang', 'stiff', 'panel', 'ratio', 'width', 'stiffen', 'space', 'graphic', 'form']}\n"
     ]
    }
   ],
   "source": [
    "print(parsed[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
